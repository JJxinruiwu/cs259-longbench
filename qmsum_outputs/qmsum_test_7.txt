Professor D: OK.
PhD A: Mike. Mike - one?
PhD B: Ah.
Professor D: We're on? Yes, please. I mean, we're testing noise robustness but let's not get silly. OK, so, uh, you've got some, uh, Xerox things to pass out?
PhD A: Yeah,
Professor D: That are {disfmarker}
PhD A: um.
Professor D: Yeah.
PhD A: Yeah. Yeah, I'm sorry for the table, but as it grows in size, uh, it.
Professor D: Uh, so for th the last column we use our imagination. OK.
PhD B: Ah, yeah.
Professor D: Ah.
PhD A: Uh, yeah.
PhD B: Uh, do you want @ @.
Professor D: This one's nice, though. This has nice big font.
PhD A: Yeah.
Grad C: Let's see. Yeah. Chop!
Professor D: Yeah.
PhD A: So
Professor D: When you get older you have these different perspectives. I mean, lowering the word hour rate is fine, but having big font!
PhD A: Next time we will put colors or something.
Professor D: That's what's {disfmarker}
PhD A: Uh.
Professor D: Yeah. It's mostly big font. OK.
PhD A: OK, s so there is kind of summary of what has been done {disfmarker}
Professor D: Uh {disfmarker} Go ahead.
PhD A: It's this. Summary of experiments since, well, since last week
Professor D: Oh. OK.
PhD A: and also since the {disfmarker} we've started to run {disfmarker} work on this. Um. {pause} So since last week we've started to fill the column with um {vocalsound} uh features w with nets trained on PLP with on - line normalization but with delta also, because the column was not completely {disfmarker}
Professor D: Mm - hmm. Mm - hmm. 
PhD A: well, it's still not completely filled,
Professor D: 
PhD A: but {pause} we have more results to compare with network using without PLP and {pause} finally, hhh, {comment} um {pause} ehhh {comment} PL - uh delta seems very important. Uh {pause} I don't know. If you take um, let's say, anyway Aurora - two - B, so, the next {disfmarker} t the second, uh, part of the table,
Professor D: Mm - hmm.
PhD A: uh {pause} when we use the large training set using French, Spanish, and English, you have one hundred and six without delta and eighty - nine with the delta.
Professor D: a And again all of these numbers are with a hundred percent being, uh, the baseline performance,
PhD A: Yeah, on the baseline, yeah. So {disfmarker}
Professor D: but with a mel cepstra system going straight into the HTK?
PhD A: Yeah. Yeah. So now we see that the gap between the different training set is much {pause} uh uh much smaller
Professor D: Yes.
PhD A: um {disfmarker}
Grad C: It's out of the way.
PhD A: But, actually, um, for English training on TIMIT is still better than the other languages. And  Mmm, {pause} Yeah. And f also for Italian, actually. If you take the second set of experiment for Italian, so, the mismatched condition,
Professor D: Mm - hmm.
PhD A: um {pause} when we use the training on TIMIT so, it's multi - English, we have a ninety - one number,
Professor D: Mm - hmm.
PhD A: and training with other languages is a little bit worse.
Professor D: Um {disfmarker} Oh, I see. Down near the bottom of this sheet.
PhD A: So,
Professor D: Uh, {comment} {pause} yes.
PhD A: yeah.
Professor D: OK.
PhD A: And, yeah, and here the gap is still more important between using delta and not using delta. If y if I take the training s the large training set, it's {disfmarker} we have one hundred and seventy - two,
Professor D: Yes.
PhD A: and one hundred and four when we use delta.
Professor D: Yeah.
PhD A: Uh. {pause} Even if the contexts used is quite the same,
Professor D: Mm - hmm.
PhD A: because without delta we use seventeenths {disfmarker} seventeen frames. Uh. Yeah, um, so the second point is that we have no single cross - language experiments, uh, that we did not have last week. Uh, so this is training the net on French only, or on English only, and testing on Italian.
Professor D: Mm - hmm.
PhD A: And training the net on French only and Spanish only and testing on, uh TI - digits.
Professor D: Mm - hmm.
PhD A: And, fff {comment} um, yeah. What we see is that these nets are not as good, except for the multi - English, which is always one of the best. Yeah, then we started to work on a large dat database containing, uh, sentences from the French, from the Spanish, from the TIMIT, from SPINE, uh from {comment} uh English digits, and from Italian digits. So this is the {disfmarker} another line {disfmarker} another set of lines in the table. Uh, @ @ with SPINE
Professor D: Ah, yes. Mm - hmm.
PhD A: and {pause} uh, actually we did this before knowing the result of all the data, uh, so we have to to redo the uh {disfmarker} the experiment training the net with, uh PLP, but with delta. But
Professor D: Mm - hmm.
PhD A: um this {disfmarker} this net performed quite well. Well, it's {disfmarker} it's better than the net using French, Spanish, and English only. Uh. So, uh, yeah. We have also started feature combination experiments. Uh many experiments using features and net outputs together. And this is {disfmarker} The results are on the other document. Uh, we can discuss this after, perhaps {disfmarker} well, just, @ @. Yeah, so basically there are four {disfmarker} four kind of systems. The first one, yeah, is combining, um, two feature streams, uh using {disfmarker} and each feature stream has its own MPL. So it's the {disfmarker} kind of similar to the tandem that was proposed for the first. The multi - stream tandem for the first proposal. The second is using features and KLT transformed MLP outputs. And the third one is to u use a single KLT trans transform features as well as MLP outputs. Um, yeah. Mmm. You know you can {disfmarker} you can comment these results,
PhD B: Yes, I can s I would like to say that, for example, um, mmm, if we doesn't use the delta - delta, uh we have an improve when we use s some combination. But when
PhD A: Yeah, we ju just to be clear, the numbers here are uh recognition accuracy.
PhD B: w Yeah, this {disfmarker} Yeah, this number recognition acc
PhD A: So it's not the {disfmarker} {vocalsound} Again we switch to another {disfmarker}
PhD B: Yes, and the baseline {disfmarker} the baseline have {disfmarker} i is eighty - two.
Professor D: Baseline is eighty - two.
PhD B: Yeah
PhD A: So it's experiment only on the Italian mismatched for the moment for this.
Professor D: Uh, this is Italian mismatched.
PhD A: Um.
PhD B: Yeah, by the moment.
PhD A: Mm - hmm.
Professor D: OK.
PhD B: And first in the experiment - one I {disfmarker} I do {disfmarker} I {disfmarker} I use different MLP,
Professor D: Mm - hmm.
PhD B: and is obviously that the multi - English MLP is the better. Um. for the ne {disfmarker} rest of experiment I use multi - English, only multi - English. And I try to combine different type of feature, but the result is that the MSG - three feature doesn't work for the Italian database because never help to increase the accuracy.
PhD A: Yeah, eh, actually, if w we look at the table, the huge table, um, we see that for TI - digits MSG perform as well as the PLP,
Professor D: Mm - hmm.
PhD A: but this is not the case for Italian what {disfmarker} where the error rate is c is almost uh twice the error rate of PLP.
Professor D: Mm - hmm.
PhD A: So, um {vocalsound} uh, well, I don't think this is a bug but this {disfmarker} this is something in {disfmarker} probably in the MSG um process that uh I don't know what exactly. Perhaps the fact that the {disfmarker} the {disfmarker} there's no low - pass filter, well, or no pre - emp pre - emphasis filter and that there is some DC offset in the Italian, or, well, something simple like that. But {disfmarker} that we need to sort out if want to uh get improvement by combining PLP and MSG
Professor D: Mm - hmm.
PhD A: because for the moment MSG do doesn't bring much information.
Professor D: Mm - hmm.
PhD A: And as Carmen said, if we combine the two, we have the result, basically, of PLP.
Professor D: I Um, the uh, baseline system {disfmarker} when you said the baseline system was uh, uh eighty - two percent, that was trained on what and tested on what? That was, uh Italian mismatched d uh, uh, digits, uh, is the testing,
PhD B: Yeah.
Professor D: and the training is Italian digits?
PhD B: Yeah.
Professor D: So the " mismatch " just refers to the noise and {disfmarker} and, uh microphone and so forth,
PhD A: Yeah.
PhD B: Yeah.
Professor D: right? So, um did we have {disfmarker} So would that then correspond to the first line here of where the training is {disfmarker} is the uh Italian digits?
PhD B: The train the training of the HTK?
Professor D: The {disfmarker}
PhD B: Yes. Ah yes!
Professor D: Yes.
PhD B: This h Yes. Th - Yes.
Professor D: Yes. Training of the net,
PhD B: Yeah.
Professor D: yeah. So, um {disfmarker} So what that says is that in a matched condition, {vocalsound} we end up with a fair amount worse putting in the uh PLP. Now w would {disfmarker} do we have a number, I suppose for the matched {disfmarker} I {disfmarker} I don't mean matched, but uh use of Italian {disfmarker} training in Italian digits for PLP only?
PhD B: Uh {pause} yes?
PhD A: Uh {pause} yeah, so this is {disfmarker} basically this is in the table. Uh {pause} so the number is fifty - two,
PhD B: Another table.
PhD A: uh {disfmarker}
Professor D: Fifty - two percent.
PhD A: Fift - So {disfmarker} No, it's {disfmarker} it's the {disfmarker}
PhD B: No.
Professor D: No, fifty - two percent of eighty - two?
PhD A: Of {disfmarker} of {disfmarker} of uh {pause} eighteen {disfmarker}
PhD B: Eighty.
PhD A: of eighteen.
PhD B: Eighty.
PhD A: So it's {disfmarker} it's error rate, basically.
PhD B: It's plus six.
PhD A: It's er error rate ratio. So {disfmarker} 
Professor D: Oh this is accuracy! 
PhD A: Uh, so we have nine {disfmarker} nine {disfmarker} let's say ninety percent.
PhD B: Yeah.
Professor D: Oy! {comment} OK. Ninety.
PhD A: Yeah. Um {comment} which is uh {comment} what we have also if use PLP and MSG together,
Professor D: Yeah.
PhD A: eighty - nine point seven.
Professor D: OK, so even just PLP, uh, it is not, in the matched condition {disfmarker} Um I wonder if it's a difference between PLP and mel cepstra, or whether it's that the net half, for some reason, is not helping.
PhD A: Uh. P - PLP and Mel cepstra give the same {disfmarker} same results.
Professor D: Same result pretty much?
PhD A: Well, we have these results. I don't know. It's not {disfmarker} Do you have this result with PLP alone, {comment} j fee feeding HTK?
Professor D: So, s
PhD A: That {disfmarker} That's what you mean?
PhD B: Yeah,
PhD A: Just PLP at the input of HTK.
PhD B: yeah yeah yeah yeah, at the first {disfmarker} and the {disfmarker} Yeah.
PhD A: Yeah. So, PLP {disfmarker}
Professor D: Eighty - eight point six.
PhD A: Yeah.
Professor D: Um, so adding MSG
PhD A: Um {disfmarker}
Professor D: um {disfmarker} Well, but that's {disfmarker} yeah, that's without the neural net,
PhD A: Yeah, that's without the neural net
Professor D: right?
PhD A: and that's the result basically that OGI has also with the MFCC with on - line normalization.
Professor D: But she had said eighty - two.
PhD A: This is the {disfmarker} w well, but this is without on - line normalization.
Professor D: Right? Oh, this {disfmarker} the eighty - two.
PhD A: Yeah.
PhD B: 
PhD A: Eighty - two is the {disfmarker} it's the Aurora baseline, so MFCC. Then we can use {disfmarker} well, OGI, they use MFCC {disfmarker} th the baseline MFCC plus on - line normalization
Professor D: Oh, I'm sorry, I k I keep getting confused because this is accuracy.
PhD A: Yeah, sorry. Yeah.
PhD B: Yeah.
Professor D: OK. Alright.
PhD A: Yeah.
Professor D: Alright. So this is {disfmarker} I was thinking all this was worse. OK so this is all better
PhD B: Yes, better.
Professor D: because eighty - nine is bigger than eighty - two.
PhD A: Mm - hmm.
PhD B: Yeah.
Professor D: OK. I'm {disfmarker} I'm all better now. OK, go ahead.
PhD A: So what happ what happens is that when we apply on - line normalization we jump to almost ninety percent.
Professor D: Yeah. Mm - hmm.
PhD A: Uh, when we apply a neural network, is the same. We j jump to ninety percent.
PhD B: Nnn, we don't know exactly.
Professor D: Yeah.
PhD A: And {disfmarker} And um {disfmarker} whatever the normalization, actually. If we use n neural network, even if the features are not correctly normalized, we jump to ninety percent. So {disfmarker}
Professor D: So we go from eighty - si eighty - eight point six to {disfmarker} to ninety, or something.
PhD A: Well, ninety {disfmarker} No, I {disfmarker} I mean ninety It's around eighty - nine, ninety, eighty - eight.
Professor D: Eighty - nine.
PhD A: Well, there are minor {disfmarker} minor differences.
PhD B: Yeah.
Professor D: And then adding the MSG does nothing, basically.
PhD A: No.
Professor D: Yeah. OK.
PhD A: Uh For Italian, yeah.
Professor D: For this case, right?
PhD A: Um.
Professor D: Alright. So, um {disfmarker} So actually, the answer for experiments with one is that adding MSG, if you {disfmarker} uh does not help in that case.
PhD A: Mm - hmm.
Professor D: Um {disfmarker}
PhD A:  But w Yeah.
Professor D: The other ones, we 'd have to look at it, but {disfmarker} And the multi - English, does uh {disfmarker} So if we think of this in error rates, we start off with, uh eighteen percent error rate, roughly.
PhD A: Mm - hmm.
Professor D: Um {pause} and {pause} we uh almost, uh cut that in half by um putting in the on - line normalization and the neural net.
PhD A: Yeah
Professor D: And the MSG doesn't however particularly affect things.
PhD A: No.
Professor D: And we cut off, I guess about twenty - five percent of the error. Uh {pause} no, not quite that, is it. Uh, two point six out of eighteen. About, um {pause} sixteen percent or something of the error, um, if we use multi - English instead of the matching condition.
PhD A: Mm - hmm. Yeah.
Professor D: Not matching condition, but uh, the uh, Italian training.
PhD A: Mm - hmm.
PhD B: Yeah.
Professor D: OK.
PhD A: Mmm.
PhD B: We select these {disfmarker} these {disfmarker} these tasks because it's the more difficult.
Professor D: Yes, good. OK? So then you're assuming multi - English is closer to the kind of thing that you could use since you're using a larger dataset, and you have more experience with it.
PhD A: Mm